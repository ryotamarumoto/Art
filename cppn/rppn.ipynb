{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d649faf-a2a7-47ac-98b4-55317b7bb414",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ops'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mRPPN\u001b[39;00m():\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ops'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Implementation of Compositional Pattern Producing Networks in Tensorflow\n",
    "\n",
    "https://en.wikipedia.org/wiki/Compositional_pattern-producing_network\n",
    "\n",
    "@hardmaru, 2016\n",
    "@w4nderlust, 2017\n",
    "\n",
    "'''\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from ops import *\n",
    "import math\n",
    "\n",
    "class RPPN():\n",
    "    def __init__(self, batch_size=1, z_dim=32, c_dim=1, scale=8.0, net_size=32, act='tanh', **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        z_dim: how many dimensions of the latent space vector (R^z_dim)\n",
    "        c_dim: 1 for mono, 3 for rgb.  dimension for output space.  you can modify code to do HSV rather than RGB.\n",
    "        net_size: number of nodes for each fully connected layer of cppn\n",
    "        scale: the bigger, the more zoomed out the picture becomes\n",
    "\n",
    "        \"\"\"\n",
    "        self.batch_size = batch_size\n",
    "        self.net_size = net_size\n",
    "        self.act = act\n",
    "        x_dim = 256\n",
    "        y_dim = 256\n",
    "        self.x_dim = x_dim\n",
    "        self.y_dim = y_dim\n",
    "        self.scale = scale\n",
    "        self.c_dim = c_dim\n",
    "        self.z_dim = z_dim\n",
    "\n",
    "        # tf Graph batch of image (batch_size, height, width, depth)\n",
    "        self.batch = tf.placeholder(tf.float32, [batch_size, x_dim, y_dim, c_dim])\n",
    "\n",
    "        n_points = x_dim * y_dim\n",
    "        self.n_points = n_points\n",
    "\n",
    "        self.x_vec, self.y_vec, self.r_vec = self._coordinates(x_dim, y_dim, scale)\n",
    "\n",
    "        # latent vector\n",
    "        self.z = tf.placeholder(tf.float32, [self.batch_size, self.z_dim])\n",
    "        # inputs to cppn, like coordinates and radius from centre\n",
    "        self.x = tf.placeholder(tf.float32, [self.batch_size, None, 1])\n",
    "        self.y = tf.placeholder(tf.float32, [self.batch_size, None, 1])\n",
    "        self.r = tf.placeholder(tf.float32, [self.batch_size, None, 1])\n",
    "        # input for number of repetitions\n",
    "        self.k = tf.placeholder(tf.int32)\n",
    "\n",
    "        # builds the generator network\n",
    "        self.G = self.generator(x_dim=self.x_dim, y_dim=self.y_dim, act=act)\n",
    "\n",
    "        self.init()\n",
    "\n",
    "    def init(self):\n",
    "        # Initializing the tensor flow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        # Launch the session\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(init)\n",
    "\n",
    "    def reinit(self):\n",
    "        init = tf.variables_initializer(tf.trainable_variables())\n",
    "        self.sess.run(init)\n",
    "\n",
    "    def _coordinates(self, x_dim=32, y_dim=32, scale=1.0):\n",
    "        '''\n",
    "        calculates and returns a vector of x and y coordintes, and corresponding radius from the centre of image.\n",
    "        '''\n",
    "        n_points = x_dim * y_dim\n",
    "        # creates x and y ranges of x/y_dim numbers from -scale to +scale\n",
    "        x_range = scale * ((np.arange(x_dim) - (x_dim - 1) / 2.0) / (x_dim - 1) * 2)\n",
    "        y_range = scale * ((np.arange(y_dim) - (y_dim - 1) / 2.0) / (y_dim - 1) * 2)\n",
    "        # create all r distances from center for any combination of coordinates on x and y\n",
    "        x_mat = np.matmul(np.ones((y_dim, 1)), x_range.reshape((1, x_dim)))\n",
    "        y_mat = np.matmul(y_range.reshape((y_dim, 1)), np.ones((1, x_dim)))\n",
    "        r_mat = np.sqrt(x_mat * x_mat + y_mat * y_mat)\n",
    "        # transform the x x y matrices tiling as many of them as the batch size\n",
    "        # and reshaping to obtain a Ã¨batch_size, x*y, 1+ tensor\n",
    "        x_mat = np.tile(x_mat.flatten(), self.batch_size).reshape(self.batch_size, n_points, 1)\n",
    "        y_mat = np.tile(y_mat.flatten(), self.batch_size).reshape(self.batch_size, n_points, 1)\n",
    "        r_mat = np.tile(r_mat.flatten(), self.batch_size).reshape(self.batch_size, n_points, 1)\n",
    "        return x_mat, y_mat, r_mat\n",
    "\n",
    "    def generator(self, x_dim, y_dim, act='tanh', reuse=False):\n",
    "        if reuse:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "        if not hasattr(tf.nn, act):\n",
    "            print(\"No activation {} available, using default tanh\".format(act))\n",
    "            act = 'tanh'\n",
    "\n",
    "        net_size = self.net_size\n",
    "        n_points = x_dim * y_dim\n",
    "\n",
    "        # note that latent vector z is scaled to self.scale factor.\n",
    "        z_scaled = tf.reshape(self.z, [self.batch_size, 1, self.z_dim]) * \\\n",
    "                   tf.ones([n_points, 1], dtype=tf.float32) * self.scale\n",
    "        z_unroll = tf.reshape(z_scaled, [self.batch_size * n_points, self.z_dim])\n",
    "        x_unroll = tf.reshape(self.x, [self.batch_size * n_points, 1])\n",
    "        y_unroll = tf.reshape(self.y, [self.batch_size * n_points, 1])\n",
    "        r_unroll = tf.reshape(self.r, [self.batch_size * n_points, 1])\n",
    "\n",
    "        sum_input = fully_connected(z_unroll, net_size, 'g_0_z') + \\\n",
    "                    fully_connected(x_unroll, net_size, 'g_0_x', with_bias=False) + \\\n",
    "                    fully_connected(y_unroll, net_size, 'g_0_y', with_bias=False) + \\\n",
    "                    fully_connected(r_unroll, net_size, 'g_0_r', with_bias=False)\n",
    "\n",
    "        hidden = tf.nn.tanh(sum_input)\n",
    "\n",
    "        i = tf.constant(0)\n",
    "        zero = tf.constant(0)\n",
    "\n",
    "        def condition(i, k, H):\n",
    "            return tf.less(i, k)\n",
    "\n",
    "        def body(i, k, H):\n",
    "            hidden = tf.cond(tf.equal(i, zero), \n",
    "                lambda: single_iteration(H, net_size, act, reuse=False), \n",
    "                lambda: single_iteration(H, net_size, act, reuse=True))\n",
    "            i = tf.add(i, 1)\n",
    "            return i, k, hidden\n",
    "\n",
    "        i, k, hidden = tf.while_loop(condition, body, [i, self.k, hidden])\n",
    "\n",
    "        output = tf.sigmoid(fully_connected(hidden, self.c_dim, 'g_final'))\n",
    "\n",
    "        '''\n",
    "        The final hidden later is pass through a fully connected sigmoid later, so outputs -> (0, 1)\n",
    "        Also, the output has a dimension of c_dim, so can be monotone or RGB\n",
    "        '''\n",
    "        result = tf.reshape(output, [self.batch_size, y_dim, x_dim, self.c_dim])\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def generate(self, z=None, x_dim=26, y_dim=26, scale=8.0, k=3, act=None, **kwargs):\n",
    "        \"\"\" Generate data by sampling from latent space.\n",
    "        If z is not None, data for this point in latent space is\n",
    "        generated. Otherwise, z is drawn from prior in latent\n",
    "        space.\n",
    "        \"\"\"\n",
    "        if z is None:\n",
    "            z = np.random.uniform(-1.0, 1.0, size=(self.batch_size, self.z_dim)).astype(np.float32)\n",
    "        # Note: This maps to mean of distribution, we could alternatively\n",
    "        # sample from Gaussian distribution\n",
    "\n",
    "        if act is None:\n",
    "            if self.act is not None:\n",
    "                act = self.act\n",
    "            else:\n",
    "                act = 'tanh'\n",
    "\n",
    "        G = self.generator(x_dim=x_dim, y_dim=y_dim, act=act, reuse=True)\n",
    "        x_vec, y_vec, r_vec = self._coordinates(x_dim, y_dim, scale=scale)\n",
    "        image = self.sess.run(G, feed_dict={self.z: z, self.x: x_vec, self.y: y_vec, self.r: r_vec, self.k: k})\n",
    "        return image\n",
    "\n",
    "    def close(self):\n",
    "        self.sess.close()\n",
    "\n",
    "\n",
    "def single_iteration(H, net_size, act='tanh', reuse=False):\n",
    "    with tf.variable_scope(\"iter_norm\", reuse=reuse):\n",
    "        H = tf.contrib.layers.layer_norm(H)\n",
    "    with tf.variable_scope(\"iter_linear\", reuse=reuse):\n",
    "        #H = fully_connected(H, net_size)\n",
    "        H = cos_sim(H, net_size)\n",
    "    H = getattr(tf.nn, act)(H)\n",
    "    return H\n",
    "\n",
    "\n",
    "def cos_sim(x, net_size, name=None):\n",
    "    with tf.name_scope(name):\n",
    "        weights = tf.get_variable(\"weights\", [net_size, net_size],\n",
    "            initializer=tf.random_normal_initializer())\n",
    "        biases = tf.get_variable(\"biases\", [net_size],\n",
    "            initializer=tf.constant_initializer(0.001))\n",
    "        wat = 0.001\n",
    "        w_norm = tf.sqrt(tf.reduce_sum(weights**2, axis=0, keep_dims=True) + biases**2)\n",
    "        x_norm = tf.sqrt(tf.reduce_sum(x**2, axis=1, keep_dims=True) + wat**2)\n",
    "        cos_sim = (tf.matmul(x, weights) + wat * biases) / w_norm / x_norm\n",
    "        return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ec5ca-6c0e-44f6-879c-3e0b2109740b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
